% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SOptim_ClassificationFunctions.R
\name{calibrateClassifier}
\alias{calibrateClassifier}
\title{Train and evaluate a classification algorithm}
\usage{
calibrateClassifier(
  calData,
  classificationMethod = "RF",
  classificationMethodParams = NULL,
  balanceTrainData = FALSE,
  balanceMethod = "ubOver",
  evalMethod = "HOCV",
  evalMetric = "Kappa",
  trainPerc = 0.8,
  nRounds = 20,
  minTrainCases = 30,
  minCasesByClassTrain = 10,
  minCasesByClassTest = 10,
  runFullCalibration = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{calData}{An input calibration dataset used for classification. It can be either an object of class \code{matrix}, 
\code{data.frame} or \code{SOptim.CalData} generated by \code{prepareCalData} (to use with option \code{runFullCalibration=TRUE}; 
see below for more details). If the input is a \code{matrix} or \code{data.frame} then it must contain one column named \code{SID} with 
segment IDs, one column named \code{"train"} defining the labels, classes or categories (either {0,1} for single-class problems, 
or, {1, 2, ..., n} for multi-class problems) followed by n columns containing features (or variables) used for training the classifier.}

\item{classificationMethod}{An input string defining the classification algorithm to be used. Available options are: 
\code{"RF"} (random forests), \code{"GBM"} (generalized boosted models), \code{"SVM"} (support vector machines), \code{"KNN"} 
(k-nearest neighbour), and, \code{"FDA"} (flexible discriminant analysis).}

\item{classificationMethodParams}{A list object with a customized set of parameters to be used for the classification algorithms 
(default = NULL). See also \link{generateDefaultClassifierParams} to see which parameters can be changed and how to structure the 
list object.}

\item{balanceTrainData}{Defines if data balancing is to be used (only available for single-class problems; default: TRUE).}

\item{balanceMethod}{A character string used to set the data balancing method. Available methods are based on under-sampling 
\code{"ubUnder"} or over-sampling \code{"ubOver"} the target class.}

\item{evalMethod}{A character string defining the evaluation method. The available methods are \code{"10FCV"} (10-fold 
cross-validation; the default), \code{"5FCV"} (5-fold cross-validation), \code{"HOCV"} (holdout cross-validation with 
the training percentage defined by \code{trainPerc} and the number of rounds defined in \code{nRounds}), and, \code{"OOB"} 
(out-of-bag evaluation; only applicable to random forests).}

\item{evalMetric}{A character string setting the evaluation metric or a function that calculates the performance score 
based on two vectors one for observed and the other for predicted values (see below for more details). 
This option defines the outcome value of the genetic algorithm fitness function and the output of grid or random search 
optimization routines. Check \code{\link{evalPerformanceGeneric}} for available options. When \code{runFullCalibration=TRUE} 
this metric will be calculated however other evaluation metrics can be quantified using \link{evalPerformanceClassifier}.}

\item{trainPerc}{A decimal number defining the training proportion (default: 0.8; if \code{"HOCV"} is used).}

\item{nRounds}{Number of training rounds used for holdout cross-validation (default: 20; if \code{"HOCV"} is used).}

\item{minTrainCases}{The minimum number of training cases used for calibration (default: 20). If the number of rows 
in \code{x} is below this number then \code{calibrateClassifier} will not run.}

\item{minCasesByClassTrain}{Minimum number of cases by class for each train data split so that the classifier 
is able to run.}

\item{minCasesByClassTest}{Minimum number of cases by class for each test data split so that the classifier 
is able to run.}

\item{runFullCalibration}{Run full calibration? Check \strong{details} section (default: FALSE).}

\item{verbose}{Print progress messages? (default: TRUE)}
}
\value{
If \code{runFullCalibration = FALSE} then a single average value (across evaluation replicates/folds) for the 
selected evaluation metric will be returned (typically used for GA optimization).       
If \code{runFullCalibration = TRUE} then an object of class \code{SOptim.Classifier} is returned with the 
following elements:      
\itemize{
   \item \bold{AvgPerf} - average value of the evaluation metric selected;
   \item \bold{PerfStats} - numeric vector with performance statistics (for the selected metric) for each 
   evaluation round plus one more round using the "full" train dataset;
   \item \bold{Thresh} - for single-class problems only; numeric vector with the threshold values (one for 
   each round plus the "full" dataset) that maximize the selected evaluation metric;
   \item \bold{ConfMat} - a list object with confusion matrices generated at each round; for single-class problems 
   this matrix is generated by dichotomizing the probability predictions (into {0,1}) using the threshold that 
   optimizes the selected evaluation metric (see 'Thresh' above);
   \item \bold{obsTestSet} - observed values for the test set (one integer vector for each evaluation round plus 
   the full evaluation round);
   \item \bold{predTestSet} - predicted values for the test set (one integer or numeric vector for each evaluation 
   round plus the full evaluation round);
   \item \bold{TrainSets} - a list object with row indices identifying train splits for each test round;
   \item \bold{ClassObj} - a list containing classifier objects for each round;
   \item \bold{ClassParams} - classification parameters used for running \code{\link{calibrateClassifier}}. 
}
}
\description{
Main function used for classifier training and evaluation for both single and multi-class problems.
}
\details{
Two working modes can be used:   
\enumerate{
   \item i) for "internal" GA optimization or grid/random search: \code{runFullCalibration = FALSE}, or,     
   \item ii) for performing a full segmented image classification: \code{runFullCalibration = TRUE}.
}
Tipically, the first option is used internally for optimizing segmentation parameters in 
\code{\link{gaOptimizeSegmentationParams}} where the output value from the selected evaluation metric 
is passed as the fitnes function outcome for \pkg{GA} optimization.        
The second option, should be used to perform a final image classification and to get full evaluation 
statistics (slot: 'PerfStats'), confusion matrices (slot: 'ConfMat'), train/test partion sets (slot: 'TrainSets'), 
classifier objects (slot: 'ClassObj') and parameters (slot: 'ClassParams'). In addition to the evaluation rounds 
(depending on the evaluation method selected) this option will also run a "full" round where all the data (i.e., 
no train/test split) will be used for training. Results from this option can then be used in \code{\link{predictSegments}}.  
This function can also perform data balancing for single-class problems (check out option \code{balanceTrainData} and 
\code{balanceMethod}). 

Check \link[unbalanced]{ubBalance} function for further details regarding data balancing.    

For more details about the classification algorithms check out the following functions: 
\itemize{
  \item \link[randomForest]{randomForest} for random forest algorithm, 
  \item \link[gbm]{gbm} for generalized boosted modelling, 
  \item \link[e1071]{svm} for details related to support vector machines, 
  \item \link[class]{knn} for k-nearest neighbour classification, and, 
  \item \link[mda]{fda} for flexible discriminant analysis. 
}
}
\note{
1) By default, if 25\% or more of the calibration/evaluation rounds must produce valid results otherwise the 
optimization algorithm will return \code{NA}.   

2) Data balancing is only performed on the train dataset to avoid bias in performance evaluation derived from 
this procedure.
}
\section{Defining custom performance evaluation functions}{


In argument \code{evalMetric} it is possible to define a custom function. This must take two vectors: one containing 
observed/ground-truth values (first argument) and other with predicted values by the trained classifier (second argument) 
and both for the test set (from holdout or k-fold CV). If the classification task is single-class (e.g., 1:forest/0:non-forest, 
1:water/0:non-water) then the predicted values will be probabilities (ranging in [0,1]) for the interest class (coded as 
1's). If the task is multi-class, then the predicted values will be integer codes for each class.

To be considered valid, the evaluation function for single-class must have:

\itemize{
   \item Have at least two inputs arguments (observed and predicted);
   \item Produce a non-null and valid numerical result;
   \item A scalar output;
   \item An attribute named \code{'thresh'} defining the numerical threshold to 
   binarize  the classifier predictions (i.e., to convert from continuous probability 
   to discrete {0,1}). The calculation of this threshold is necessary to maximize the 
   value of the performance metric instead of using a naive 0.5 cutoff value.
}

Here goes an example function used to calculate the maximum value for the overall 
accuracy based on multiple threshold values:

\preformatted{

calcMaxAccuracy <- function(obs, pred){

   accuracies <- c()
   i <- 0
   N <- length(obs)
   thresholds <- seq(0, 1, 0.05)

   for(thresh in thresholds){
      i <- i + 1   
      pred_bin <- as.integer(pred > thresh)
      confusionMatrix <- as.matrix(table(obs, pred_bin))
      accuracies[i] <- diag(confusionMatrix) / N
   }

   bestAccuracy <- max(accuracies)
   attr(bestAccuracy, "thresh") <- thresholds[which.max(accuracies)]

   return(bestAccuracy)
}

x <- sample(0:1,100,replace=TRUE)
y <- runif(100)
calcMaxAccuracy(obs = x, pred = y)

} 

Valid multi-class functions' must have:

\itemize{
   \item Have at least two inputs arguments (observed and predicted);
   \item Produce a non-null and valid numerical result;
   \item A scalar output.
}

An example of a valid custom function to calculate the overall accuracy:

\preformatted{

calcAccuracy <- function(obs, pred){

   N <- length(obs)
   confusionMatrix <- as.matrix(table(obs, pred))
   acc <- diag(confusionMatrix) / N
   return(acc)
}

x <- sample(0:1,100,replace=TRUE)
y <- sample(0:1,100,replace=TRUE)
calcAccuracy(obs = x, pred = y)

}
}

